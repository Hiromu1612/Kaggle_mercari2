{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eba89b61-80bb-47ca-9bb7-e9e32a2d3ce7",
   "metadata": {},
   "source": [
    "#### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3ba2857d-97f8-4bb6-90fa-0f6c9e520462",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.pipeline import FeatureUnion\n",
    "from sklearn.model_selection import train_test_split\n",
    "import gc\n",
    "from sklearn.linear_model import Ridge, RidgeCV\n",
    "from sklearn.metrics import mean_squared_log_error\n",
    "import warnings\n",
    "warnings.simplefilter('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2c1aa47-5f55-4672-a0ee-928d8fe91097",
   "metadata": {},
   "source": [
    "#### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4b77b3d4-f669-42b2-8ea8-0baf61a07ce1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1482535, 8)\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1482535 entries, 0 to 1482534\n",
      "Data columns (total 8 columns):\n",
      " #   Column             Non-Null Count    Dtype  \n",
      "---  ------             --------------    -----  \n",
      " 0   train_id           1482535 non-null  int64  \n",
      " 1   name               1482535 non-null  object \n",
      " 2   item_condition_id  1482535 non-null  int64  \n",
      " 3   category_name      1476208 non-null  object \n",
      " 4   brand_name         849853 non-null   object \n",
      " 5   price              1482535 non-null  float64\n",
      " 6   shipping           1482535 non-null  int64  \n",
      " 7   item_description   1482529 non-null  object \n",
      "dtypes: float64(1), int64(3), object(4)\n",
      "memory usage: 90.5+ MB\n",
      "None\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>train_id</th>\n",
       "      <th>name</th>\n",
       "      <th>item_condition_id</th>\n",
       "      <th>category_name</th>\n",
       "      <th>brand_name</th>\n",
       "      <th>price</th>\n",
       "      <th>shipping</th>\n",
       "      <th>item_description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>MLB Cincinnati Reds T Shirt Size XL</td>\n",
       "      <td>3</td>\n",
       "      <td>Men/Tops/T-shirts</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10.0</td>\n",
       "      <td>1</td>\n",
       "      <td>No description yet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Razer BlackWidow Chroma Keyboard</td>\n",
       "      <td>3</td>\n",
       "      <td>Electronics/Computers &amp; Tablets/Components &amp; P...</td>\n",
       "      <td>Razer</td>\n",
       "      <td>52.0</td>\n",
       "      <td>0</td>\n",
       "      <td>This keyboard is in great condition and works ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>AVA-VIV Blouse</td>\n",
       "      <td>1</td>\n",
       "      <td>Women/Tops &amp; Blouses/Blouse</td>\n",
       "      <td>Target</td>\n",
       "      <td>10.0</td>\n",
       "      <td>1</td>\n",
       "      <td>Adorable top with a hint of lace and a key hol...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Leather Horse Statues</td>\n",
       "      <td>1</td>\n",
       "      <td>Home/Home Décor/Home Décor Accents</td>\n",
       "      <td>NaN</td>\n",
       "      <td>35.0</td>\n",
       "      <td>1</td>\n",
       "      <td>New with tags. Leather horses. Retail for [rm]...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>24K GOLD plated rose</td>\n",
       "      <td>1</td>\n",
       "      <td>Women/Jewelry/Necklaces</td>\n",
       "      <td>NaN</td>\n",
       "      <td>44.0</td>\n",
       "      <td>0</td>\n",
       "      <td>Complete with certificate of authenticity</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   train_id                                 name  item_condition_id  \\\n",
       "0         0  MLB Cincinnati Reds T Shirt Size XL                  3   \n",
       "1         1     Razer BlackWidow Chroma Keyboard                  3   \n",
       "2         2                       AVA-VIV Blouse                  1   \n",
       "3         3                Leather Horse Statues                  1   \n",
       "4         4                 24K GOLD plated rose                  1   \n",
       "\n",
       "                                       category_name brand_name  price  \\\n",
       "0                                  Men/Tops/T-shirts        NaN   10.0   \n",
       "1  Electronics/Computers & Tablets/Components & P...      Razer   52.0   \n",
       "2                        Women/Tops & Blouses/Blouse     Target   10.0   \n",
       "3                 Home/Home Décor/Home Décor Accents        NaN   35.0   \n",
       "4                            Women/Jewelry/Necklaces        NaN   44.0   \n",
       "\n",
       "   shipping                                   item_description  \n",
       "0         1                                 No description yet  \n",
       "1         0  This keyboard is in great condition and works ...  \n",
       "2         1  Adorable top with a hint of lace and a key hol...  \n",
       "3         1  New with tags. Leather horses. Retail for [rm]...  \n",
       "4         0          Complete with certificate of authenticity  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_train = pd.read_csv('data/train.tsv', sep='\\t', low_memory=True)\n",
    "print(df_train.shape)\n",
    "print(df_train.info())\n",
    "display(df_train.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "369d1558-4564-4477-bdaf-3e01fb6e3b30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(693359, 7)\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 693359 entries, 0 to 693358\n",
      "Data columns (total 7 columns):\n",
      " #   Column             Non-Null Count   Dtype \n",
      "---  ------             --------------   ----- \n",
      " 0   test_id            693359 non-null  int64 \n",
      " 1   name               693359 non-null  object\n",
      " 2   item_condition_id  693359 non-null  int64 \n",
      " 3   category_name      690301 non-null  object\n",
      " 4   brand_name         397834 non-null  object\n",
      " 5   shipping           693359 non-null  int64 \n",
      " 6   item_description   693359 non-null  object\n",
      "dtypes: int64(3), object(4)\n",
      "memory usage: 37.0+ MB\n",
      "None\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>test_id</th>\n",
       "      <th>name</th>\n",
       "      <th>item_condition_id</th>\n",
       "      <th>category_name</th>\n",
       "      <th>brand_name</th>\n",
       "      <th>shipping</th>\n",
       "      <th>item_description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Breast cancer \"I fight like a girl\" ring</td>\n",
       "      <td>1</td>\n",
       "      <td>Women/Jewelry/Rings</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>Size 7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>25 pcs NEW 7.5\"x12\" Kraft Bubble Mailers</td>\n",
       "      <td>1</td>\n",
       "      <td>Other/Office supplies/Shipping Supplies</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>25 pcs NEW 7.5\"x12\" Kraft Bubble Mailers Lined...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Coach bag</td>\n",
       "      <td>1</td>\n",
       "      <td>Vintage &amp; Collectibles/Bags and Purses/Handbag</td>\n",
       "      <td>Coach</td>\n",
       "      <td>1</td>\n",
       "      <td>Brand new coach bag. Bought for [rm] at a Coac...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Floral Kimono</td>\n",
       "      <td>2</td>\n",
       "      <td>Women/Sweaters/Cardigan</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>-floral kimono -never worn -lightweight and pe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Life after Death</td>\n",
       "      <td>3</td>\n",
       "      <td>Other/Books/Religion &amp; Spirituality</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>Rediscovering life after the loss of a loved o...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   test_id                                      name  item_condition_id  \\\n",
       "0        0  Breast cancer \"I fight like a girl\" ring                  1   \n",
       "1        1  25 pcs NEW 7.5\"x12\" Kraft Bubble Mailers                  1   \n",
       "2        2                                 Coach bag                  1   \n",
       "3        3                             Floral Kimono                  2   \n",
       "4        4                          Life after Death                  3   \n",
       "\n",
       "                                    category_name brand_name  shipping  \\\n",
       "0                             Women/Jewelry/Rings        NaN         1   \n",
       "1         Other/Office supplies/Shipping Supplies        NaN         1   \n",
       "2  Vintage & Collectibles/Bags and Purses/Handbag      Coach         1   \n",
       "3                         Women/Sweaters/Cardigan        NaN         0   \n",
       "4             Other/Books/Religion & Spirituality        NaN         1   \n",
       "\n",
       "                                    item_description  \n",
       "0                                             Size 7  \n",
       "1  25 pcs NEW 7.5\"x12\" Kraft Bubble Mailers Lined...  \n",
       "2  Brand new coach bag. Bought for [rm] at a Coac...  \n",
       "3  -floral kimono -never worn -lightweight and pe...  \n",
       "4  Rediscovering life after the loss of a loved o...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_test = pd.read_csv('data/test.tsv', sep='\\t', low_memory=True)\n",
    "print(df_test.shape)\n",
    "print(df_test.info())\n",
    "display(df_test.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef7f0728-c4cf-4432-879b-e2a12f025448",
   "metadata": {},
   "source": [
    "#### Count the number of words in the product name and product description\n",
    "商品名と商品説明の単語の数を調べる"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b135c696-ca84-450f-8659-1e70c7552605",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 10.3 s\n",
      "Wall time: 10.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "def wordCount(text):\n",
    "    \"\"\"\n",
    "    Parameters:\n",
    "      text(str): 商品名、商品の説明文\n",
    "    \"\"\"\n",
    "    try:\n",
    "        if text == 'No description yet':\n",
    "            return 0  # 商品名や説明が'No description yet'の場合は0を返す\n",
    "        else:\n",
    "            text = text.lower()                  # すべて小文字にする\n",
    "            words = [w for w in text.split(\" \")] # スペースで切り分ける\n",
    "            return len(words)                    # 単語の数を返す\n",
    "    except: \n",
    "        return 0\n",
    "\n",
    "# 'name'の各フィールドの単語数を'name_len'に登録\n",
    "df_train['name_len'] = df_train['name'].apply(lambda x: wordCount(x))\n",
    "df_test['name_len'] = df_test['name'].apply(lambda x: wordCount(x))\n",
    "# 'item_description'の各フィールドの単語数を'desc_len'に登録\n",
    "df_train['desc_len'] = df_train['item_description'].apply(lambda x: wordCount(x))\n",
    "df_test['desc_len'] = df_test['item_description'].apply(lambda x: wordCount(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49ad45b4-f857-4fce-b8e4-aa7e340a1e4a",
   "metadata": {},
   "source": [
    "#### Scaling of the selling price\n",
    "販売価格の分布が正規分布になるように対数変換を行う"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9ba83eaf-a872-4107-b68d-830df443fffb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 46.9 ms\n",
      "Wall time: 34 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# 訓練データの'price'を対数変換する\n",
    "df_train[\"target\"] = np.log1p(df_train.price)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b618ba1d-0696-43ee-88b5-ef23b97d0de0",
   "metadata": {},
   "source": [
    "#### Splitting the category name and registering it in a new column\n",
    "カテゴリ名を切り分けて新設のカラムに登録する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "44e8701a-be18-4fd3-8a00-1eab7b889a40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 10.4 s\n",
      "Wall time: 10.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "def split_cat(text):\n",
    "    \"\"\"\n",
    "    Parameters:\n",
    "      text(str): カテゴリ名\n",
    "\n",
    "    ・カテゴリを/で切り分ける\n",
    "    ・データが存在しない場合は\"No Label\"を返す\n",
    "    \"\"\"\n",
    "    try: return text.split(\"/\")\n",
    "    except: return (\"No Label\", \"No Label\", \"No Label\")\n",
    "\n",
    "# 3つに切り分けたカテゴリ名を'subcat_0'、'subcat_1'、'subcat_2'に登録\n",
    "# 訓練データ\n",
    "df_train['subcat_0'], df_train['subcat_1'], df_train['subcat_2'] = \\\n",
    "    zip(*df_train['category_name'].apply(lambda x: split_cat(x)))\n",
    "# テストデータ\n",
    "df_test['subcat_0'], df_test['subcat_1'], df_test['subcat_2'] = \\\n",
    "    zip(*df_test['category_name'].apply(lambda x: split_cat(x)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97469a54-2409-4c98-a9c2-981e7617b33b",
   "metadata": {},
   "source": [
    "#### Replace the missing values of the brand name with meaningful data\n",
    "ブランド名の欠損値を意味のあるデータに置き換える"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "acff7517-fe1c-46d8-b574-dcab2d60fe19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "632682\n",
      "137418\n",
      "295525\n",
      "64154\n",
      "CPU times: total: 3min 1s\n",
      "Wall time: 3min 6s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# df_trainとdf_testを縦方向に結合\n",
    "full_set = pd.concat([df_train, df_test])\n",
    "# full_setの'brand_name'から重複なしのブランドリスト(集合)を生成\n",
    "all_brands = set(full_set['brand_name'].values)\n",
    "\n",
    "# 'brand_name'の欠損値NaNを'missing'に置き換える\n",
    "df_train['brand_name'].fillna(value='missing', inplace=True)\n",
    "df_test['brand_name'].fillna(value='missing', inplace=True)\n",
    "\n",
    "# 訓練データの'brand_name'が'missing'に一致するレコード数を取得\n",
    "train_premissing = len(df_train.loc[df_train['brand_name'] == 'missing'])\n",
    "# テストデータの'brand_name'が'missing'に一致するレコード数を取得\n",
    "test_premissing = len(df_test.loc[df_test['brand_name'] == 'missing'])\n",
    "\n",
    "def brandfinder(line):\n",
    "    \"\"\"\n",
    "    Parameters: line(str): ブランド名\n",
    "\n",
    "    ・ブランド名の'missing'を商品名に置き換える:\n",
    "         missing'の商品名の単語がブランドリストに存在する場合\n",
    "    ・ブランド名を商品名に置き換える:\n",
    "        商品名がブランドリストの名前と完全に一致する場合\n",
    "    ・ブランド名をそのままにする:\n",
    "        商品名がブランドリストの名前と一致しない\n",
    "        商品名が'missing'だが商品名の単語がブランドリストにない\n",
    "    \"\"\"\n",
    "    brand = line[0] # 第1要素はブランド名\n",
    "    name = line[1]  # 第2要素は商品名\n",
    "    namesplit = name.split(' ') # 商品名をスペースで切り分ける\n",
    "    \n",
    "    if brand == 'missing':  # ブランド名が'missing'と一致\n",
    "        for x in namesplit: # 商品名から切り分けた単語を取り出す\n",
    "            if x in all_brands:                \n",
    "                return name # 単語がブランドリストに一致したら商品名を返す\n",
    "    if name in all_brands:  # 商品名がブランドリストに存在すれば商品名を返す\n",
    "        return name\n",
    "    \n",
    "    return brand            # どれにも一致しなければブランド名を返す\n",
    "\n",
    "# ブランド名の付替えを実施\n",
    "df_train['brand_name'] = df_train[['brand_name','name']].apply(brandfinder, axis = 1)\n",
    "df_test['brand_name'] = df_test[['brand_name','name']].apply(brandfinder, axis = 1)\n",
    "\n",
    "# 書き換えられた'missing'の数を取得\n",
    "train_found = train_premissing-len(df_train.loc[df_train['brand_name'] == 'missing'])\n",
    "test_found = test_premissing-len(df_test.loc[df_test['brand_name'] == 'missing'])\n",
    "print(train_premissing) # 書き換える前の'missing'の数\n",
    "print(train_found)      # 書き換えられた'missing'の数\n",
    "print(test_premissing)  # 書き換える前の'missing'の数\n",
    "print(test_found)       # 書き換えられた'missing'の数"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96ee208c-0479-4f12-aec7-38ff8946a94c",
   "metadata": {},
   "source": [
    "#### Split the training data into training and validation sets\n",
    "訓練用のデータフレームを訓練用と検証用に99:1で分割する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "268aa858-6553-4a84-9142-ef287e3ef5c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training : 1467709 examples\n",
      "Validating : 14826 examples\n",
      "Testing : 693359 examples\n",
      "CPU times: total: 3.78 s\n",
      "Wall time: 5.39 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "train_dfs, dev_dfs = train_test_split(\n",
    "    df_train,         # 対象のデータフレーム\n",
    "    random_state=123, # 乱数生成時のシード(種)\n",
    "    train_size=0.99,  # 訓練用に99%のデータ\n",
    "    test_size=0.01)   # 検証用に1%のデータ\n",
    "\n",
    "n_trains = train_dfs.shape[0] # 訓練データのサイズ\n",
    "n_devs = dev_dfs.shape[0]     # 検証データのサイズ\n",
    "n_tests = df_test.shape[0]   # テストデータのサイズ\n",
    "print('Training :', n_trains, 'examples')\n",
    "print('Validating :', n_devs, 'examples')\n",
    "print('Testing :', n_tests, 'examples')\n",
    "\n",
    "del df_train\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea1bb3e0-c1e6-4956-a28a-14a0ca41ef19",
   "metadata": {},
   "source": [
    "#### Handle the missing values and convert all the data to strings\n",
    "欠損値を処理し、すべてのデータを文字列にする"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "83601e4e-61d7-4a79-9b3a-7873cf10f2c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Handling missing values...\n",
      "CPU times: total: 7.77 s\n",
      "Wall time: 7.99 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# 訓練データ、検証データ、テストデータを1つのデータフレームに連結\n",
    "df_all = pd.concat([train_dfs, dev_dfs, df_test])\n",
    "\n",
    "print(\"Handling missing values...\")\n",
    "# カテゴリ名の欠損値を'missing'に置き換える\n",
    "df_all['category_name'] = \\\n",
    "    df_all['category_name'].fillna('missing').astype(str)\n",
    "# サブカテゴリのラベルを文字列に変換\n",
    "df_all['subcat_0'] = df_all['subcat_0'].astype(str)\n",
    "df_all['subcat_1'] = df_all['subcat_1'].astype(str)\n",
    "df_all['subcat_2'] = df_all['subcat_2'].astype(str)\n",
    "# ブランド名の欠損値を'missing'に置き換える\n",
    "df_all['brand_name'] = df_all['brand_name'].fillna('missing').astype(str)\n",
    "# 送料負担、商品の状態を文字列に置き換える\n",
    "df_all['shipping'] = df_all['shipping'].astype(str)\n",
    "df_all['item_condition_id'] = df_all['item_condition_id'].astype(str)\n",
    "# 説明文の単語数、商品名の単語数を文字列に置き換える\n",
    "df_all['desc_len'] = df_all['desc_len'].astype(str)\n",
    "df_all['name_len'] = df_all['name_len'].astype(str)\n",
    "# 説明文の欠損値を'No description yet'に置き換える\n",
    "df_all['item_description'] = \\\n",
    "    df_all['item_description'].fillna('No description yet').astype(str)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5594cf48-dee6-467f-9b06-65d5a716c69d",
   "metadata": {},
   "source": [
    "#### Vectorize all the data using Bag-of-Words\n",
    "Bag-of-WordsとN-gramでテキストデータをベクトル化する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "70f3c7ed-8f8b-44b4-94e0-c971277b4805",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vectorizing data...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[1;32m<timed exec>:63\u001b[0m\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\ds2024\\lib\\site-packages\\sklearn\\utils\\_set_output.py:295\u001b[0m, in \u001b[0;36m_wrap_method_output.<locals>.wrapped\u001b[1;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[0;32m    293\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(f)\n\u001b[0;32m    294\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapped\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m--> 295\u001b[0m     data_to_wrap \u001b[38;5;241m=\u001b[39m f(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    296\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_to_wrap, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[0;32m    297\u001b[0m         \u001b[38;5;66;03m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[0;32m    298\u001b[0m         return_tuple \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    299\u001b[0m             _wrap_data_with_container(method, data_to_wrap[\u001b[38;5;241m0\u001b[39m], X, \u001b[38;5;28mself\u001b[39m),\n\u001b[0;32m    300\u001b[0m             \u001b[38;5;241m*\u001b[39mdata_to_wrap[\u001b[38;5;241m1\u001b[39m:],\n\u001b[0;32m    301\u001b[0m         )\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\ds2024\\lib\\site-packages\\sklearn\\pipeline.py:1618\u001b[0m, in \u001b[0;36mFeatureUnion.fit_transform\u001b[1;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[0;32m   1597\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfit_transform\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, y\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params):\n\u001b[0;32m   1598\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Fit all transformers, transform the data and concatenate results.\u001b[39;00m\n\u001b[0;32m   1599\u001b[0m \n\u001b[0;32m   1600\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1616\u001b[0m \u001b[38;5;124;03m        sum of `n_components` (output dimension) over transformers.\u001b[39;00m\n\u001b[0;32m   1617\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1618\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_parallel_func\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfit_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_fit_transform_one\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1619\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m results:\n\u001b[0;32m   1620\u001b[0m         \u001b[38;5;66;03m# All transformers are None\u001b[39;00m\n\u001b[0;32m   1621\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39mzeros((X\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;241m0\u001b[39m))\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\ds2024\\lib\\site-packages\\sklearn\\pipeline.py:1642\u001b[0m, in \u001b[0;36mFeatureUnion._parallel_func\u001b[1;34m(self, X, y, fit_params, func)\u001b[0m\n\u001b[0;32m   1638\u001b[0m transformers \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iter())\n\u001b[0;32m   1640\u001b[0m params \u001b[38;5;241m=\u001b[39m Bunch(fit\u001b[38;5;241m=\u001b[39mfit_params, fit_transform\u001b[38;5;241m=\u001b[39mfit_params)\n\u001b[1;32m-> 1642\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mParallel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_jobs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1643\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1644\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtransformer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1645\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1646\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1647\u001b[0m \u001b[43m        \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1648\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmessage_clsname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mFeatureUnion\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1649\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmessage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_log_message\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtransformers\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1650\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1651\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1652\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtransformer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtransformers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1653\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\ds2024\\lib\\site-packages\\sklearn\\utils\\parallel.py:67\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m     62\u001b[0m config \u001b[38;5;241m=\u001b[39m get_config()\n\u001b[0;32m     63\u001b[0m iterable_with_config \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m     64\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[0;32m     65\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[0;32m     66\u001b[0m )\n\u001b[1;32m---> 67\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43miterable_with_config\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\ds2024\\lib\\site-packages\\joblib\\parallel.py:1863\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1861\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_sequential_output(iterable)\n\u001b[0;32m   1862\u001b[0m     \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[1;32m-> 1863\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1865\u001b[0m \u001b[38;5;66;03m# Let's create an ID that uniquely identifies the current call. If the\u001b[39;00m\n\u001b[0;32m   1866\u001b[0m \u001b[38;5;66;03m# call is interrupted early and that the same instance is immediately\u001b[39;00m\n\u001b[0;32m   1867\u001b[0m \u001b[38;5;66;03m# re-used, this id will be used to prevent workers that were\u001b[39;00m\n\u001b[0;32m   1868\u001b[0m \u001b[38;5;66;03m# concurrently finalizing a task from the previous call to run the\u001b[39;00m\n\u001b[0;32m   1869\u001b[0m \u001b[38;5;66;03m# callback.\u001b[39;00m\n\u001b[0;32m   1870\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\ds2024\\lib\\site-packages\\joblib\\parallel.py:1792\u001b[0m, in \u001b[0;36mParallel._get_sequential_output\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1790\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_dispatched_batches \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m   1791\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_dispatched_tasks \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m-> 1792\u001b[0m res \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1793\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_completed_tasks \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m   1794\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprint_progress()\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\ds2024\\lib\\site-packages\\sklearn\\utils\\parallel.py:129\u001b[0m, in \u001b[0;36m_FuncWrapper.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    127\u001b[0m     config \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m    128\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mconfig):\n\u001b[1;32m--> 129\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunction(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\ds2024\\lib\\site-packages\\sklearn\\pipeline.py:1303\u001b[0m, in \u001b[0;36m_fit_transform_one\u001b[1;34m(transformer, X, y, weight, message_clsname, message, params)\u001b[0m\n\u001b[0;32m   1301\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _print_elapsed_time(message_clsname, message):\n\u001b[0;32m   1302\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(transformer, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfit_transform\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m-> 1303\u001b[0m         res \u001b[38;5;241m=\u001b[39m transformer\u001b[38;5;241m.\u001b[39mfit_transform(X, y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfit_transform\u001b[39m\u001b[38;5;124m\"\u001b[39m, {}))\n\u001b[0;32m   1304\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1305\u001b[0m         res \u001b[38;5;241m=\u001b[39m transformer\u001b[38;5;241m.\u001b[39mfit(X, y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfit\u001b[39m\u001b[38;5;124m\"\u001b[39m, {}))\u001b[38;5;241m.\u001b[39mtransform(\n\u001b[0;32m   1306\u001b[0m             X, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtransform\u001b[39m\u001b[38;5;124m\"\u001b[39m, {})\n\u001b[0;32m   1307\u001b[0m         )\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\ds2024\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:2138\u001b[0m, in \u001b[0;36mTfidfVectorizer.fit_transform\u001b[1;34m(self, raw_documents, y)\u001b[0m\n\u001b[0;32m   2131\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_params()\n\u001b[0;32m   2132\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tfidf \u001b[38;5;241m=\u001b[39m TfidfTransformer(\n\u001b[0;32m   2133\u001b[0m     norm\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm,\n\u001b[0;32m   2134\u001b[0m     use_idf\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muse_idf,\n\u001b[0;32m   2135\u001b[0m     smooth_idf\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msmooth_idf,\n\u001b[0;32m   2136\u001b[0m     sublinear_tf\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msublinear_tf,\n\u001b[0;32m   2137\u001b[0m )\n\u001b[1;32m-> 2138\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mraw_documents\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2139\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tfidf\u001b[38;5;241m.\u001b[39mfit(X)\n\u001b[0;32m   2140\u001b[0m \u001b[38;5;66;03m# X is already a transformed view of raw_documents so\u001b[39;00m\n\u001b[0;32m   2141\u001b[0m \u001b[38;5;66;03m# we set copy to False\u001b[39;00m\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\ds2024\\lib\\site-packages\\sklearn\\base.py:1474\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1467\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1469\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1470\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1471\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1472\u001b[0m     )\n\u001b[0;32m   1473\u001b[0m ):\n\u001b[1;32m-> 1474\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fit_method(estimator, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\ds2024\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:1389\u001b[0m, in \u001b[0;36mCountVectorizer.fit_transform\u001b[1;34m(self, raw_documents, y)\u001b[0m\n\u001b[0;32m   1381\u001b[0m             warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m   1382\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUpper case characters found in\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1383\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m vocabulary while \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlowercase\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1384\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m is True. These entries will not\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1385\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m be matched with any documents\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1386\u001b[0m             )\n\u001b[0;32m   1387\u001b[0m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m-> 1389\u001b[0m vocabulary, X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_count_vocab\u001b[49m\u001b[43m(\u001b[49m\u001b[43mraw_documents\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfixed_vocabulary_\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1391\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbinary:\n\u001b[0;32m   1392\u001b[0m     X\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mfill(\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\ds2024\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:1289\u001b[0m, in \u001b[0;36mCountVectorizer._count_vocab\u001b[1;34m(self, raw_documents, fixed_vocab)\u001b[0m\n\u001b[0;32m   1287\u001b[0m     j_indices\u001b[38;5;241m.\u001b[39mextend(feature_counter\u001b[38;5;241m.\u001b[39mkeys())\n\u001b[0;32m   1288\u001b[0m     values\u001b[38;5;241m.\u001b[39mextend(feature_counter\u001b[38;5;241m.\u001b[39mvalues())\n\u001b[1;32m-> 1289\u001b[0m     \u001b[43mindptr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mappend\u001b[49m(\u001b[38;5;28mlen\u001b[39m(j_indices))\n\u001b[0;32m   1291\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m fixed_vocab:\n\u001b[0;32m   1292\u001b[0m     \u001b[38;5;66;03m# disable defaultdict behaviour\u001b[39;00m\n\u001b[0;32m   1293\u001b[0m     vocabulary \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m(vocabulary)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%%time\n",
    "print(\"Vectorizing data...\")\n",
    "# CountVectorizerの生成処理を関数化\n",
    "default_preprocessor = CountVectorizer().build_preprocessor()\n",
    "\n",
    "def build_preprocessor(field):\n",
    "    \"\"\"\n",
    "    指定されたカラムのインデックスを取得し、\n",
    "    トークンカウント行列を作成するためのCountVectorizerを返す\n",
    "    \n",
    "    Parameter:field(str)\n",
    "      フル結合データフレームのカラム名\n",
    "    \"\"\"\n",
    "    field_idx = list(df_all.columns).index(field)\n",
    "    return lambda x: default_preprocessor(x[field_idx])\n",
    "\n",
    "# トークンカウント行列\n",
    "# CountVectorizeを結合して\n",
    "# (識別子, ベクトライザーオブジェクト)のリストで構成される\n",
    "# トランスファーマーオブジェクトを生成\n",
    "vectorizer = FeatureUnion([\n",
    "    ('name', CountVectorizer(\n",
    "        # bag-of-wordで分割する単位をn-gramで連続する単語のつながりとする\n",
    "        # 商品名は2-gram(2つの単語のつながり)で分割\n",
    "        ngram_range=(1, 2),\n",
    "        max_features=1000, # トークンカウントの上限値############### 50000\n",
    "        # トークンカウントステップをオーバーライド\n",
    "        preprocessor=build_preprocessor('name'))),\n",
    "    ('subcat_0', CountVectorizer(\n",
    "        # トークンの構成を示す正規表現を'+'として1文字に対応\n",
    "        token_pattern='.+',\n",
    "        preprocessor=build_preprocessor('subcat_0'))),\n",
    "    ('subcat_1', CountVectorizer(\n",
    "        token_pattern='.+',\n",
    "        preprocessor=build_preprocessor('subcat_1'))),\n",
    "    ('subcat_2', CountVectorizer(\n",
    "        token_pattern='.+',\n",
    "        preprocessor=build_preprocessor('subcat_2'))),\n",
    "    ('brand_name', CountVectorizer(\n",
    "        token_pattern='.+',\n",
    "        preprocessor=build_preprocessor('brand_name'))),\n",
    "    ('shipping', CountVectorizer(\n",
    "        token_pattern='\\d+',\n",
    "        preprocessor=build_preprocessor('shipping'))),\n",
    "    ('item_condition_id', CountVectorizer(\n",
    "        token_pattern='\\d+',\n",
    "        preprocessor=build_preprocessor('item_condition_id'))),\n",
    "    ('desc_len', CountVectorizer(\n",
    "        token_pattern='\\d+',\n",
    "        preprocessor=build_preprocessor('desc_len'))),\n",
    "    ('name_len', CountVectorizer(\n",
    "        token_pattern='\\d+',\n",
    "        preprocessor=build_preprocessor('name_len'))),\n",
    "    ('item_description', TfidfVectorizer(\n",
    "        # bag-of-wordで分割する単位をn-gramで連続する単語のつながりとする\n",
    "        # 商品説名は3-gram(3つの単語のつながり)で分割\n",
    "        ngram_range=(1, 3),\n",
    "        max_features=1000, # トークンカウントの上限値#################### 100000\n",
    "        preprocessor=build_preprocessor('item_description'))),\n",
    "])\n",
    "\n",
    "# フル結合のデータフレームのフィールド値を\n",
    "# n-grainによるbag-of-wordsでトークンカウント行列に変換する\n",
    "X = vectorizer.fit_transform(df_all.values)\n",
    "\n",
    "del vectorizer\n",
    "gc.collect()\n",
    "\n",
    "# 入力用のdictオブジェクトから訓練用のデータを抽出\n",
    "X_train = X[:n_trains]\n",
    "# 訓練用のデータフレームから商品価格を抽出して\n",
    "# (データ数, 価格)の2階テンソルに変換\n",
    "Y_train = train_dfs.target.values.reshape(-1, 1)\n",
    "\n",
    "# 入力用のdictオブジェクトから検証用のデータを抽出\n",
    "X_dev = X[n_trains:n_trains+n_devs]\n",
    "# 検証用のデータフレームから商品価格を抽出して\n",
    "# (データ数, 価格)の2階テンソルに変換\n",
    "Y_dev = dev_dfs.target.values.reshape(-1, 1)\n",
    "\n",
    "# 入力用のdictオブジェクトからテストデータを抽出\n",
    "X_test = X[n_trains+n_devs:]\n",
    "\n",
    "print('X:', X.shape)\n",
    "print('X_train:', X_train.shape)\n",
    "print('X_dev:', X_dev.shape)\n",
    "print('X_test:', X_test.shape)\n",
    "print('Y_train:', Y_train.shape)\n",
    "print('Y_dev:', Y_dev.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c0a77e9-9996-4812-b4ac-ec713106de9c",
   "metadata": {},
   "source": [
    "#### Optimizing with Ridge and RidgeCV\n",
    "Ridge、RidgeCVでそれぞれ線形回帰を行う"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e899fdf1-f88b-4a85-b57f-ad84a0c249a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "print(\"Fitting Ridge model on training examples...\")\n",
    "ridge_model = Ridge(\n",
    "    solver='auto',      # ソルバーをオートモードにする\n",
    "    fit_intercept=True, # 切片を計算に使用\n",
    "    alpha=1.0,          # 正則化の強度はデフォルト値\n",
    "    max_iter=200,       # ソルバーの最大反復回数\n",
    "    normalize=False,    # 正規化を行う\n",
    "    tol=0.01,           # 回帰の反復を停止するときの精度\n",
    "    # データをシャッフルするときに使用する疑似乱数ジェネレータのシード\n",
    "    random_state = 1,\n",
    ")\n",
    "\n",
    "print(\"Fitting RidgeCV model on training examples...\")\n",
    "ridge_modelCV = RidgeCV(\n",
    "    fit_intercept=True, # 切片を計算に使用\n",
    "    alphas=[5.0],\n",
    "    normalize=False,\n",
    "    cv = 2, # 交差検証時にスコアを2回連続して(毎回異なる分割で)計算\n",
    "    # モデルの評価は平均2乗誤差回帰損失で行う\n",
    "    scoring='neg_mean_squared_error',\n",
    ")\n",
    "\n",
    "ridge_model.fit(X_train, Y_train)\n",
    "ridge_modelCV.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4adda7a-bd4b-487c-ad86-735351bfdc79",
   "metadata": {},
   "source": [
    "#### Measuring the loss by inputting the validation data into the Ridge model\n",
    "Ridgeモデルに検証データを入力して損失を測定"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3adf0962-eb80-41af-8b5b-6d947acb6426",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_dev_preds_ridge = ridge_model.predict(X_dev)\n",
    "Y_dev_preds_ridge = Y_dev_preds_ridge.reshape(-1, 1)\n",
    "print('Ridge model RMSL error:', mean_squared_log_error(Y_dev, Y_dev_preds_ridge))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "205ce067-c926-4648-b45a-d1c6f9eae86d",
   "metadata": {},
   "source": [
    "#### Measuring the loss by inputting the validation data into the RidgeCV model\n",
    "RidgeCVモデルに検証データを入力して損失を測定"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "812017ec-36ea-4e18-a42a-0443242c3473",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RidgeCVモデルに検証データを入力して損失を測定\n",
    "Y_dev_preds_ridgeCV = ridge_modelCV.predict(X_dev)\n",
    "Y_dev_preds_ridgeCV = Y_dev_preds_ridgeCV.reshape(-1, 1)\n",
    "print('RidgeCV model RMSL error:', mean_squared_log_error(Y_dev, Y_dev_preds_ridgeCV))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dc53052-363f-4783-a398-2a04bf954889",
   "metadata": {},
   "source": [
    "#### Prediction of test data\n",
    "テストデータで予測する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f73d1d9-da5a-498a-8b2a-6a8c749c67dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Ridgeモデル\n",
    "ridge_preds = ridge_model.predict(X_test)\n",
    "ridge_preds = np.expm1(ridge_preds)\n",
    "\n",
    "# RidgeCVモデル\n",
    "ridgeCV_preds = ridge_modelCV.predict(X_test)\n",
    "ridgeCV_preds = np.expm1(ridgeCV_preds)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
